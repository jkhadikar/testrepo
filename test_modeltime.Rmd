---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(tidymodels)
library(modeltime)
library(lubridate)
library(timetk)

# This toggles plots from plotly (interactive) to ggplot (static)
itact <- FALSE

```

# Step 1 - Collect data and split into training and test sets.

```{r}
# Data
m750 <- m4_monthly %>% filter(id == "M750")
```

```{r}
m750 %>%
  plot_time_series(date, value, .interactive = itact)
```

```{r}
# Split Data 80/20
splits <- initial_time_split(m750, prop = 0.9)
```
#Step 2 - Create & Fit Multiple Models
```{r}
# Model 1: auto_arima ----
model_fit_arima_no_boost <- arima_reg() %>%
    set_engine(engine = "auto_arima") %>%
    fit(value ~ date, data = training(splits))
#> frequency = 12 observations per 1 year


# Model 2: arima_boost ----
model_fit_arima_boosted <- arima_boost(
    min_n = 2,
    learn_rate = 0.015
) %>%
    set_engine(engine = "auto_arima_xgboost") %>%
    fit(value ~ date + as.numeric(date) + factor(month(date, label = TRUE), ordered = F),
        data = training(splits))
#> frequency = 12 observations per 1 year

# Model 3: ets ----
model_fit_ets <- exp_smoothing() %>%
    set_engine(engine = "ets") %>%
    fit(value ~ date, data = training(splits))
#> frequency = 12 observations per 1 year


# Model 4: prophet ----
model_fit_prophet <- prophet_reg() %>%
    set_engine(engine = "prophet") %>%
    fit(value ~ date, data = training(splits))
#> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.
#> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.

# Model 5: lm ----
model_fit_lm <- linear_reg() %>%
    set_engine("lm") %>%
    fit(value ~ as.numeric(date) + factor(month(date, label = TRUE), ordered = FALSE),
        data = training(splits))

# Model 6: earth ----
model_spec_mars <- mars(mode = "regression") %>%
    set_engine("earth") 

recipe_spec <- recipe(value ~ date, data = training(splits)) %>%
    step_date(date, features = "month", ordinal = FALSE) %>%
    step_mutate(date_num = as.numeric(date)) %>%
    step_normalize(date_num) %>%
    step_rm(date)
  
wflw_fit_mars <- workflow() %>%
    add_recipe(recipe_spec) %>%
    add_model(model_spec_mars) %>%
    fit(training(splits))


```
#Step 3 - Add fitted models to a Model Table
```{r}
models_tbl <- modeltime_table(
    model_fit_arima_no_boost,
    model_fit_arima_boosted,
    model_fit_ets,
    model_fit_prophet,
    model_fit_lm,
    wflw_fit_mars
)

models_tbl
```
# Step 4 - Calibrate the model to a testing set.
```{r}
calibration_tbl <- models_tbl %>%
    modeltime_calibrate(new_data = testing(splits))

calibration_tbl
```
#Step 5 - Testing Set Forecast & Accuracy Evaluation
```{r}

calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = m750
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = interactive
    )


calibration_tbl %>%
    modeltime_accuracy() %>%
    table_modeltime_accuracy(
        .interactive = interactive
    )

```
#Step 6 - Refit to Full Dataset & Forecast Forward
```{r}
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = m750)

refit_tbl %>%
    modeltime_forecast(h = "3 years", actual_data = m750) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = interactive
    )
```


```{r}


library(rvest)
library(dplyr)

link = "https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc"
page = read_html(link)

name = page %>% html_nodes(".lister-item-header a") %>% html_text()
year = page %>% html_nodes(".text-muted.unbold") %>% html_text()
rating = page %>% html_nodes(".ratings-imdb-rating strong") %>% html_text()
synopsis = page %>% html_nodes(".ratings-bar+ .text-muted") %>% html_text()

movies = data.frame(name, year, rating, synopsis, stringsAsFactors = FALSE)
write.csv(movies, "movies.csv")
```



```{r}

name1 <- link %>% 
  read_html() %>% 
  html_nodes(".lister-item-header a") %>% 
  html_text()

year1 <- link %>% 
  read_html() %>% 
  html_nodes(".text-muted.unbold") %>% 
  html_text()

rating1 <- link %>% 
  read_html() %>% 
  html_nodes(".ratings-imdb-rating strong") %>% 
  html_text()

synopsis1 <- link %>% 
  read_html() %>% 
  html_nodes(".ratings-bar+ .text-muted") %>% 
  html_text()

movies = data.frame(name, year, rating, synopsis, stringsAsFactors = FALSE)
write.csv(movies, "movies.csv")


```

```{r}
library(rvest)
library(dplyr)

link = "https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc"
page = read_html(link)

name = page %>% html_nodes(".lister-item-header a") %>% html_text()
movie_links = page %>% html_nodes(".lister-item-header a") %>%
  html_attr("href") %>% paste("https://www.imdb.com", ., sep="")
year = page %>% html_nodes(".text-muted.unbold") %>% html_text()
rating = page %>% html_nodes(".ratings-imdb-rating strong") %>% html_text()
synopsis = page %>% html_nodes(".ratings-bar+ .text-muted") %>% html_text()

get_cast = function(movie_link) {
  movie_page = read_html(movie_link)
  movie_cast = movie_page %>% html_nodes(".primary_photo+ td a") %>%
    html_text() %>% paste(collapse = ",")
  return(movie_cast)
}

cast = sapply(movie_links, FUN = get_cast, USE.NAMES = FALSE)

movies = data.frame(name, year, rating, synopsis, cast, stringsAsFactors = FALSE)
```
# SCRAPPING FROM MULTIPLE PAGES
```{r}
library(rvest)
library(dplyr)

get_cast = function(movie_link) {
  movie_page = read_html(movie_link)
  movie_cast = movie_page %>% html_nodes(".primary_photo+ td a") %>%
    html_text() %>% paste(collapse = ",")
  return(movie_cast)
}

movies = data.frame()

for (page_result in seq(from = 1, to = 51, by = 50)) {
  link = paste0("https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc&start=", 
               page_result, "&ref_=adv_nxt")
  page = read_html(link)
  
  name = page %>% html_nodes(".lister-item-header a") %>% html_text()
  movie_links = page %>% html_nodes(".lister-item-header a") %>%
    html_attr("href") %>% paste("https://www.imdb.com", ., sep="")
  year = page %>% html_nodes(".text-muted.unbold") %>% html_text()
  rating = page %>% html_nodes(".ratings-imdb-rating strong") %>% html_text()
  synopsis = page %>% html_nodes(".ratings-bar+ .text-muted") %>% html_text()
  cast = sapply(movie_links, FUN = get_cast, USE.NAMES = FALSE)
  
  
  movies = rbind(movies, data.frame(name, year, rating, synopsis, cast, stringsAsFactors = FALSE))

  print(paste("Page:", page_result))  
}
```

#WEB SCRAPPING TABLES
```{r}

library(rvest)
library(dplyr)

col_link = "https://www.patriotsoftware.com/blog/accounting/average-cost-living-by-state/"
col_page = read_html(col_link)

col_table = col_page %>% html_nodes("table#featuresGrid") %>%
  html_table() %>% .[[1]]


wiki_link = "https://en.wikipedia.org/wiki/List_of_apple_cultivars"
wiki_page = read_html(wiki_link)

apple_table = wiki_page %>% html_nodes("table") %>% .[2] %>% 
  html_table(fill = TRUE) %>% .[[1]]
```

DSDOJO - WEB SCRAPPING
```{r}
# Data Science Dojo Meetup: Automated Web Scraping in R

# Let's start scraping the main head and paragraph text/comments of a single Reddit page
#install.packages("rvest")
library(rvest)

reddit_wbpg <- read_html("https://www.reddit.com/r/politics/comments/a1j9xs/partisan_election_officials_are_inherently_unfair/")

reddit_wbpg %>%
  html_node("title") %>%
  html_text()  

reddit_wbpg %>%
  html_nodes("p.s90z9tc-10") %>%
  html_text()

# Let's scrape the time and URL of all latest pages published on Reddit's r/politics
reddit_political_news <- read_html("https://www.reddit.com/r/politics/new/")

time <- reddit_political_news %>%
  html_nodes("a._3jOxDPIQ0KaOWpzvSQo-1s") %>% 
  html_text()

time

urls <- reddit_political_news %>%
  html_nodes("a._3jOxDPIQ0KaOWpzvSQo-1s") %>%
  html_attr("href")

urls

# Create a dataframe containing the URLs of the Reddit news pages and their published times
reddit_newspgs_times <- data.frame(NewsPage=urls, PublishedTime=time)
#Check the dimensions 
dim(reddit_newspgs_times)

# Filter dataframe by rows that contain a time published in minutes (i.e. within the hour)
reddit_recent_data <- reddit_newspgs_times[grep("minute|now", reddit_newspgs_times$PublishedTime),]
#Check the dimensions (# items will be less if not all pages were published within mins)
dim(reddit_recent_data)

# Loop through urls, grab the main head and paragraph text of comments, 
# store in their own vectors, and create a dataframe to get it ready for analysis/modeling

titles <- c()
comments <- c()
for(i in reddit_recent_data$NewsPage){ 
  
  reddit_recent_data <- read_html(i)
  body <- reddit_recent_data %>%
    html_nodes("p.s90z9tc-10") %>%
    html_text()
  comments = append(comments, body)
  
  reddit_recent_data <- read_html(i)
  title <- reddit_recent_data %>%
    html_node("title") %>%
    html_text()
  titles = append(titles, rep(title,each=length(body)))
  
}

reddit_hourly_data <- data.frame(Headline=titles, Comments=comments)
dim(reddit_hourly_data)
head(reddit_hourly_data$Comments)

# Remove disclaimer comments included in all pages so this doesn't flood the comments and skew results
disclaimers <- c(
  "As a reminder, this subreddit is for civil discussion.",
  "In general, be courteous to others. Attack ideas, not users. Personal insults, shill or troll accusations, hate speech, any advocating or wishing death/physical harm, and other rule violations can result in a permanent ban.",
  "If you see comments in violation of our rules, please report them.",
  "I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns."
)

reddit_hourly_data_no_disclaimers <- subset(
  reddit_hourly_data, !(Comments %in% c(disclaimers))
)

dim(reddit_hourly_data_no_disclaimers)
head(reddit_hourly_data_no_disclaimers$Comments)

# Score the overall sentiment of each comment
# This library scores sentiment by taking into account the whole sentence
# It takes into account surrounding words of a target word such as 'not happy'
# which cancels out positive sentiment
# A negative value means sentiment is more negative than positive
# A positive values means the sentiment is more positive than negative
#install.packages('sentimentr')
library(sentimentr)

# Comment out this line so it does not cause errors when scheduling to run the script
#sentiment(reddit_hourly_data_no_disclaimers$Comments)

# Treat comments as characters, not factors
# Convert to a format sentiment() function accepts
reddit_hourly_data_no_disclaimers$Comments <- as.character(reddit_hourly_data_no_disclaimers$Comments)

sentiment_scores <- sentiment(reddit_hourly_data_no_disclaimers$Comments)
head(sentiment_scores)

# Average the scores across all comments
average_sentiment_score <- sum(sentiment_scores$sentiment)/length(sentiment_scores$sentiment)
average_sentiment_score

# Email the results of the analysis
#install.packages("sendmailR")
library(sendmailR)
from <- "<rebecca.merrett@gmail.com>"
to <- "<rebecca.merrett@gmail.com>"
subject <- "Hourly Sentiment Score on Current US Political Situation"
body <- c("On a scale of 1 to -1 people feel: ", average_sentiment_score)            
mailControl <- list(smtpServer="ASPMX.L.GOOGLE.COM") #Use Google for Gmail accounts

sendmail(from=from,to=to,subject=subject,msg=body,control=mailControl)

# Schedule this script to run every hour to keep track of the overall sentiment 
# Idea to take this further: Instead of emailing the hourly results, 
# store the average sentiment score in a table every hour to plot it 
# over time or see how changes over time

```

```{r}
# Automated Web Scraping in R

# Let's start with a quick demonstration of scraping 
# the main head and body text of a single web page 
#install.packages("rvest") #Uncomment this to install this package
library(rvest)

marketwatch_wbpg <- read_html(
  "https://www.marketwatch.com/story/bitcoin-jumps-after-credit-scare-2018-10-15"
)

marketwatch_wbpg %>%
  html_node("title") %>% #See HTML source code for data within this tag
  html_text()

marketwatch_wbpg %>%
  html_nodes("p") %>% #See HTML source code for data within this tag
  html_text()

# Let's read in all news on Bitcoin using the
# Marketwatch source
marketwatch_bitcoin_articles <- read_html(
  "https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=15&mp=0&bd=false&rs=false"
)

# Grab all datetimes on the page
datetime <- marketwatch_bitcoin_articles %>%
  html_nodes("div.deemphasized span") %>% #See HTML source code for data within this tag
  html_text()

datetime

# Filter datetimes that do not follow a consistent format
datetime2 <- c()
for(i in datetime){
  correct_datetime <- grep("Today", i, invert=T, value=T)
  datetime2 <- append(datetime2, correct_datetime)
}

datetime <- datetime2

datetime

# Convert datetime text to a standard time format
#install.packages("lubridate") #Uncomment this to install this package
library(lubridate)

# First remove periods from datetime, as lubridate 
# cannot interpret a.m. and p.m. with periods
datetime_clean <- gsub("\\.","",datetime)

datetime_parse <- parse_date_time(
  datetime_clean, "%I:%M %p %m/%d/%Y"
)
datetime_parse

# Convert all ET (Eastern Time) datetime values to 
# your local time - e.g. PT (Pacific Time)
datetime_convert <- ymd_hms(
  datetime_parse, tz = "US/Eastern"
)
datetime_convert <- with_tz(
  datetime_convert, "US/Pacific"
)
datetime_convert

# Create a dataframe containing the urls of the web 
# pages and their converted datetimes
marketwatch_webpgs_datetimes <- data.frame(
  WebPg=urls, DateTime=datetime_convert
)
dim(marketwatch_webpgs_datetimes)

# Take the difference between the your current time
# and the published datetime of the web pg and add 
# as a column to the dataframe
diff_in_hours <- difftime(
  Sys.time(), marketwatch_webpgs_datetimes$DateTime, units = "hours"
)
diff_in_hours
diff_in_hours <- as.double(diff_in_hours)
diff_in_hours
marketwatch_webpgs_datetimes$DiffHours <- diff_in_hours
head(marketwatch_webpgs_datetimes)

# Filter rows of the dataframe that contain 
# DiffHours of less than an hour
marketwatch_latest_data <- subset(
  marketwatch_webpgs_datetimes, DiffHours < 1
)
marketwatch_latest_data

# Loop through web pg URLs, read and grab the title 
# and body text, and store in a dataframe to get 
# the data ready for analysis
titles <- c()
bodies <- c()
for(i in marketwatch_latest_data$WebPg){
  
  marketwatch_latest_wbpg <- read_html(i)
  title <- marketwatch_latest_wbpg %>%
    html_node("title") %>%
    html_text()
  titles <- append(titles, title)
  
  marketwatch_latest_wbpg <- read_html(i)
  body <- marketwatch_latest_wbpg %>%
    html_nodes("p") %>%
    html_text()
  one_body <- paste(body, collapse=" ")
  bodies <- append(bodies, one_body)
  
}

marketwatch_latest_data$Title <- titles
marketwatch_latest_data$Body <- bodies

names(marketwatch_latest_data)
marketwatch_latest_data$Title
marketwatch_latest_data$Body[1]


# Summarize the body of the text to extract the most 
# relevant, key info

# Note: There are other ways to analyze the text:
# Learn text analytics/natural language processing 
# and important machine learning concepts: 
# https://datasciencedojo.com/bootcamp/curriculum/ 

# Before summarizing the text, we need to clean it 
# of uneccessary whitespace, new lines, etc 
#install.packages("stringr") #Uncomment this to install this package
library(stringr)
clean_text_bodies <- str_squish(
  marketwatch_latest_data$Body
  )
clean_text_bodies[1]

# Loop through each body text and grab the top 3 
# sentences with the most relevant information
#install.packages("LSAfun") #Uncomment this to install this package
library(LSAfun)
summary <- c()
for(i in clean_text_bodies){
  top_info <- genericSummary(i,k=3);
  one_summary <- paste(top_info, collapse=" ")
  summary <- append(summary, one_summary)
}

summary

marketwatch_latest_data$Summary <- summary

# Email the results of the summaries, along with 
# the titles
#install.packages("sendmailR") #Uncomment this to install this package
library(sendmailR)

marketwatch_title_summary <- c()
for(i in 1:length(marketwatch_latest_data$Summary)){
  marketwatch_title_summary <- append(marketwatch_title_summary, marketwatch_latest_data$Title[i])
  marketwatch_title_summary <- append(marketwatch_title_summary, marketwatch_latest_data$Summary[i])
}

marketwatch_title_summary

from <- "<rebecca.merrett@gmail.com>"
to <- "<rebecca.merrett@gmail.com>"
subject <- "Hourly Summary of Bitcoin Events"
body <- marketwatch_title_summary             
mailControl <- list(smtpServer="ASPMX.L.GOOGLE.COM") #Use Google for Gmail accounts

sendmail(from=from,to=to,subject=subject,msg=body,control=mailControl)

#Schedule this script to run every hour

```

